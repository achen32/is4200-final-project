{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/akshithabhashetty/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/akshithabhashetty/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/akshithabhashetty/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import the libraries\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from nltk.util import ngrams as n_grams\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "\n",
    "# download nltk resources for query preprocessing\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "import gensim.downloader\n",
    "import spacy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import kagglehub\n",
    "import os\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load kaggle dataset\n",
    "path = kagglehub.dataset_download(\"nikhilnayak123/5-million-song-lyrics-dataset\")\n",
    "\n",
    "# read the dataset\n",
    "data_file = os.path.join(path, os.listdir(path)[0])\n",
    "df = pd.read_csv(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for non-empty lyrics with at least some content\n",
    "df = df[df['lyrics'].apply(lambda x: isinstance(x, str) and len(x.strip()) > 50)]\n",
    "\n",
    "# take 10,000 random samples since its too large to use completely\n",
    "df_subset = df.sample(n=10000, random_state=42)\n",
    "\n",
    "# create a music corpus with tokenized words for the phrase finder\n",
    "music_corpus = [] \n",
    "# iterate through lyrics column\n",
    "for lyrics in df_subset['lyrics']:\n",
    "    if isinstance(lyrics, str):\n",
    "        # tokenize the lyrics into individual words and store\n",
    "        tokens = word_tokenize(lyrics.lower())\n",
    "\n",
    "        # only keep ASCII tokens (filtering for only english words)\n",
    "        english_tokens = []\n",
    "        for token in tokens:\n",
    "            # check if the token contains only ASCII characters \n",
    "            if all(ord(c) < 128 for c in token):\n",
    "                english_tokens.append(token)\n",
    "        \n",
    "        # if most tokens are ASCII, consider it English\n",
    "        if len(english_tokens) > (0.7 * len(tokens)):\n",
    "            music_corpus.extend(english_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a set of english words from nltk\n",
    "english_words = set(words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a set of stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# remove a small subset of descriptive words considered as stopwords\n",
    "# could be important in understanding context/intent in query\n",
    "stop_words -= {'like', 'with', 'without', 'for', 'similar', 'to', 'in', 'by', 'as'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords from tokenized query\n",
    "def remove_stopwords(tokens) :\n",
    "    filtered_tokens = []\n",
    "    # iterate through tokens and keep words that are not in the stopwords\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Porter stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# textually normalizes the tokens (removes suffixes or finds root words)\n",
    "def textually_normalize_tokens(tokens, normalizer, stem=True):\n",
    "    normalized_tokens = []\n",
    "    # iterate through token and store stemmed token\n",
    "    for token in tokens:\n",
    "        # if it is a stemmer, then stem otherwise lemmatize\n",
    "        if stem:\n",
    "            normalized_tokens.append(normalizer.stem(token))\n",
    "        else:\n",
    "            normalized_tokens.append(normalizer.lemmatize(token))\n",
    "\n",
    "    return normalized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess query at basic level with tokenization, removing stopwords, and normalizing tokens\n",
    "def preprocess_query_initial(query, normalizer, stem=True):\n",
    "    # lowercase the query and then tokenize using nltk word tokenizer\n",
    "    query_tokens = word_tokenize(query.lower())\n",
    "\n",
    "    # remove stopwords from the query tokens\n",
    "    filtered_tokens = remove_stopwords(query_tokens)\n",
    "\n",
    "    # normalize tokens with lemmatizer or stemmer\n",
    "    normalized_tokens = textually_normalize_tokens(filtered_tokens, normalizer, stem)\n",
    "\n",
    "    return normalized_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spell checking the word using levenshtein distance\n",
    "def levenshtein_autocorrect(word, threshold=2):\n",
    "    # if the word is an actual word, return it back as it was probably spelled correctly\n",
    "    if word in english_words:\n",
    "        return word\n",
    "    \n",
    "    # initialize a list of possible correct words\n",
    "    possible_corrections = []\n",
    "    # iterate through each word in the vocabulary to compare edit distance\n",
    "    for dict_word in english_words:\n",
    "        # if the difference in length between the word and the vocabulary word \n",
    "        # is less than or equal to the threshold, then check the edit distance. \n",
    "        # otherwise it is automatically not a possible correction\n",
    "        if abs(len(dict_word) - len(word)) > threshold:\n",
    "            # find the edit distance between the two words\n",
    "            distance = edit_distance(word, dict_word)\n",
    "            # if the distance is less than or equal to the threshold, store the word and the distance\n",
    "            if distance <= threshold:\n",
    "                possible_corrections.append((dict_word, distance))\n",
    "    # if there are any possible corrections found for the word, return the one with the least distance\n",
    "    if possible_corrections:\n",
    "        return sorted(possible_corrections, key=lambda x: x[1])[0][0]\n",
    "    # if no possible corrections found, return the word itself\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spell checking the query using levenshtein distance\n",
    "def autocorrect_query(tokens):\n",
    "    # iterate through the tokens and use levenshtein distance to correct if appropriate\n",
    "    for index, token in enumerate(tokens):\n",
    "        tokens[index] = levenshtein_autocorrect(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spell checking the query\n",
    "def autocorrect_query(tokens):\n",
    "    spell = SpellChecker()\n",
    "    # iterate through the tokens and if the token is mispelled (not seen in spell), \n",
    "    # then correct and replace in the list of tokens\n",
    "    for index, token in enumerate(tokens):\n",
    "        if token not in spell:\n",
    "            tokens[index] = spell.correction(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained word2vec model, potentially fine-tune later\n",
    "word2vec_model = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find similar words from the word2vec model\n",
    "def add_similar_query_tokens(tokens):\n",
    "    similar_terms = []\n",
    "    # for each token in the query, find 2 similar terms and append\n",
    "    for token in tokens:\n",
    "        # find the 2 most similar words to this token\n",
    "        similar_terms_for_token = word2vec_model.most_similar(token, 3)\n",
    "        final_similar_terms_for_token = []\n",
    "        # remove any terms that are too different (low similarity)\n",
    "        for term, similarity_score in similar_terms_for_token:\n",
    "            if len(final_similar_terms_for_token) < 2:\n",
    "                if term not in tokens and similarity_score >= 0:\n",
    "                    final_similar_terms_for_token.append(term)\n",
    "        # append the similar terms for this token to the overall list\n",
    "        similar_terms.extend(final_similar_terms_for_token)\n",
    "    # return the similar terms for this query\n",
    "    return similar_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract named entities from the query using spacy\n",
    "def extract_named_entities(query):\n",
    "    # pass query through spacy model\n",
    "    query_ner = nlp(query)\n",
    "    \n",
    "    # iterate through the entities that were recognized and store in a list\n",
    "    entities = []\n",
    "    for ent in query_ner.ents:\n",
    "        entities.append((ent.text, ent.label_))\n",
    "    \n",
    "    # return list of entities containing the actual text and then the label\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that preprocesses the query with basic steps and then enhances with autocorrect, ner, and expanded terms\n",
    "def preprocess_query_enhanced(query):\n",
    "    # first preprocess with initial steps\n",
    "    normalized_tokens = preprocess_query_initial(query, lemmatizer, stem=False)\n",
    "\n",
    "    # autocorrect the tokens to fix any mispellings\n",
    "    corrected_tokens = autocorrect_query(normalized_tokens)\n",
    "\n",
    "    # expand query with word embeddings\n",
    "    expanded_terms = add_similar_query_tokens(corrected_tokens)\n",
    "\n",
    "    # extract named entities using spacy\n",
    "    entities = extract_named_entities(query)\n",
    "\n",
    "    # add the entities to the query for extra weight\n",
    "    for entity_text, entity_type in entities:\n",
    "        normalized_tokens.append(entity_text)\n",
    "\n",
    "    return normalized_tokens, expanded_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that finds negations in the tokens\n",
    "def identify_negations(tokens):\n",
    "    # list of preliminary negation terms\n",
    "    negation_terms = {'not', 'no', \"don't\", \"doesn't\", 'never', 'without', \"won't\", 'neither', 'nor'}\n",
    "    # initialize the lists that will contain the detected negations and the corresponding tokens\n",
    "    result = {'negation_terms': [], 'negated_terms': []}\n",
    "    \n",
    "    # iterate through the tokens and check if there is a negation and do the appropriate steps\n",
    "    for index, token in enumerate(tokens):\n",
    "        # if the token is a negation, then find the tokens it corresponds to\n",
    "        if token.lower() in negation_terms:\n",
    "            # store the negation\n",
    "            result['negation_terms'].append(token)\n",
    "            # if there is a token after this one, store the token that was negated as well\n",
    "            if index + 1 < len(tokens):\n",
    "                result['negated_terms'].append(tokens[index + 1])\n",
    "    # return the negations and their corresponding terms\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that removes negated terms and negations\n",
    "def handle_negations(tokens, expanded_terms):\n",
    "    # find the negations and the terms that were negated\n",
    "    negation_info = identify_negations(tokens)\n",
    "    \n",
    "    # if there are no terms that were negated, just return the tokens again since there is nothing to be done\n",
    "    if not negation_info['negated_terms']:\n",
    "        return tokens, expanded_terms\n",
    "    \n",
    "    # find terms that must be removed from the query\n",
    "    exclude_terms = set()\n",
    "    # iterate through the negated terms, if similar ones found remove them\n",
    "    for negated_term in negation_info['negated_terms']:\n",
    "        # add the negated term\n",
    "        exclude_terms.add(negated_term)\n",
    "        # find any similar words\n",
    "        if negated_term.lower() in word2vec_model:\n",
    "            similar_words = word2vec_model.most_similar(negated_term, topn=5)\n",
    "            # add the similar words\n",
    "            for word, score in similar_words:\n",
    "                exclude_terms.add(word)\n",
    "    \n",
    "    # initialize lists\n",
    "    filtered_tokens = []\n",
    "    filtered_expanded_terms = []\n",
    "    # iterate through the tokens and remove negated terms from tokens and expanded terms \n",
    "    for token in tokens:\n",
    "        # if the token is not to be excluded, store it\n",
    "        if token not in exclude_terms:\n",
    "            filtered_tokens.append(token)\n",
    "            # if the token is not to be excluded and its an expanded term, store it\n",
    "            if token in expanded_terms:\n",
    "                filtered_expanded_terms.append(token)\n",
    "\n",
    "    return filtered_tokens, filtered_expanded_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram function\n",
    "def extract_ngrams(tokens, n=2):\n",
    "    # make a list of the ngrams\n",
    "    ngrams = list(n_grams(tokens, n))\n",
    "    # iterate through the ngrams and make each ngram a phrase or single string\n",
    "    extracted_ngrams = []\n",
    "    for ngram in ngrams:\n",
    "        extracted_ngrams.append(' '.join(ngram))\n",
    "    return extracted_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the word phrases using pointwise mutual information\n",
    "def find_phrases(tokens, top_n=3):\n",
    "    # separate all of the string tokens from any punctuation tokens\n",
    "    string_tokens = []\n",
    "    for token in tokens:\n",
    "        # if the token is not punctuation, store it\n",
    "        if token not in string.punctuation:\n",
    "            string_tokens.append(token)\n",
    "    \n",
    "    # if there are fewer than 5 tokens, there are probably no phrases so return nothing\n",
    "    if len(tokens) < 5:\n",
    "        return []\n",
    "    \n",
    "    # intialize bigram measures and finders to find words phrases of length two\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(music_corpus) \n",
    "\n",
    "    # keep pairs that appear more than once\n",
    "    finder.apply_freq_filter(15)\n",
    "    \n",
    "    # find the top n phrases\n",
    "    phrases = finder.nbest(bigram_measures.pmi, top_n * 10)\n",
    "\n",
    "    # filter for only english phrases by iterating through the phrases and using langdetect\n",
    "    english_phrases = []\n",
    "    for word1, word2 in phrases:\n",
    "        # if the phrase is english, store\n",
    "        if word1 in english_words and word2 in english_words:\n",
    "            english_phrases.append(' '.join((word1, word2)))\n",
    "\n",
    "        # if enough english phrases found, stop looking for more\n",
    "        if len(english_phrases) >= top_n:\n",
    "            return english_phrases\n",
    "\n",
    "    # return whatever is in english_phrases\n",
    "    return english_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the importance of the phrase\n",
    "def compute_phrase_importance(text, ngram):\n",
    "    # make the text and ngrams sets to get the unique words from each\n",
    "    ngram_words = set(ngram.lower().split())\n",
    "    text_words = set(text.lower().split())\n",
    "    \n",
    "    # if the words in ngram are present in the words in the text, then calculate a score as the ratio\n",
    "    if ngram_words.issubset(text_words):\n",
    "        return len(ngram_words) / len(text_words)\n",
    "    # otherwise return zero as it is not important\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand the query using the context and ngraphs\n",
    "def context_sensitive_expansion(query, expanded_terms, ngrams):\n",
    "    # filter through the expanded terms to keep only the ones that are important \n",
    "    relevant_expanded_terms = []    \n",
    "    for term in expanded_terms:\n",
    "        term_relevance = 0.0\n",
    "        # for each ngram phrase, find the importance of the query containing the expanded term to find its importance/relevance\n",
    "        for ngram in ngrams:\n",
    "            importance = compute_phrase_importance(query + \" \" + term, ngram)\n",
    "            term_relevance += importance\n",
    "        # if the term is relevant and not an ngram, then store because it is important\n",
    "        if term_relevance > 0 or not ngrams:\n",
    "            relevant_expanded_terms.append(term)\n",
    "    \n",
    "    return relevant_expanded_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess query, then enhance it, and next expand with negation handling and important phrases\n",
    "def preprocess_query_context_aware(query):\n",
    "    # do the enhanced query preprocessing\n",
    "    tokens, expanded_terms = preprocess_query_enhanced(query)\n",
    "\n",
    "    # get the ngrams and relevant word phrases\n",
    "    bigrams = extract_ngrams(tokens, 2)\n",
    "    trigrams = extract_ngrams(tokens, 3)\n",
    "    phrases = find_phrases(tokens)\n",
    "\n",
    "    # combine all these phrases\n",
    "    phrases = list(set(bigrams + trigrams + phrases))\n",
    "    \n",
    "    # handle any negations\n",
    "    filtered_tokens, filtered_expanded_terms = handle_negations(tokens, expanded_terms)\n",
    "    \n",
    "    # do the context-sensitive expansion\n",
    "    context_sensitive_expanded_terms = context_sensitive_expansion(query, filtered_expanded_terms, phrases)\n",
    "\n",
    "    # return the tokens with the additional terms/phrases\n",
    "    return filtered_tokens, context_sensitive_expanded_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the embeddings for the term\n",
    "def calculate_term_embedding(term):\n",
    "    # make sure the term is not a phrase so split in case\n",
    "    term_words = term.lower().split()\n",
    "    embeddings = []\n",
    "    # for each word in the term, find the embedding\n",
    "    for word in term_words:\n",
    "        # if there is an embedding for the word, store it\n",
    "        if word in word2vec_model:\n",
    "            embeddings.append(word2vec_model[word])\n",
    "    # if embeddings were found, find the average of them for the embedding of the entire term\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    # otherwise, just return zeroes as no embeddings were found\n",
    "    return np.zeros(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use maximal marginal relevance to find the diverse and relevant terms\n",
    "def apply_maximal_marginal_relevance(query_embedding, possible_terms, lambda_param=0.5, num_terms=10):\n",
    "    # if there are no possible terms, return nothing\n",
    "    if not possible_terms:\n",
    "        return []\n",
    "    \n",
    "    # find the embeddings of each possible term\n",
    "    possible_term_embeddings = []\n",
    "    for term in possible_terms:\n",
    "        possible_term_embeddings.append(calculate_term_embedding(term))\n",
    "    # make it an array to do the cosine similarity\n",
    "    possible_term_embeddings = np.array(possible_term_embeddings)\n",
    "    \n",
    "    # find the similarities between the possible term's embeddings and the query \n",
    "    query_similarities = cosine_similarity(possible_term_embeddings, query_embedding.reshape(1, -1)).flatten()\n",
    "    \n",
    "    # find the top num terms possible terms to use\n",
    "    selected_indices = []\n",
    "    remaining_indices = list(range(len(possible_terms)))\n",
    "    # iterate the same number of times as the number of terms wanted or all the possible terms, whichever is fewer\n",
    "    for iteration in range(min(num_terms, len(possible_terms))):\n",
    "        # if there are no more terms to check, stop\n",
    "        if not remaining_indices:\n",
    "            break\n",
    "            \n",
    "        # find the mmr scores for the remaining possible terms\n",
    "        mmr_scores = []\n",
    "        for index in remaining_indices:\n",
    "            # if there are terms that have been selected\n",
    "            if selected_indices:\n",
    "                # get their embeddings\n",
    "                selected_embeddings = possible_term_embeddings[selected_indices]\n",
    "\n",
    "                # find similarities between this possible term and those that have been selected\n",
    "                similarities_to_selected = cosine_similarity(possible_term_embeddings[index].reshape(1, -1), selected_embeddings).flatten()\n",
    "                \n",
    "                # calculate the mmr using the similarities\n",
    "                mmr = lambda_param * query_similarities[index] - (1 - lambda_param) * np.max(similarities_to_selected)\n",
    "            # otherwise just use the query similarities as the mmr\n",
    "            else:\n",
    "                mmr = query_similarities[index]\n",
    "            # store the mmr score for this possible term    \n",
    "            mmr_scores.append((index, mmr))\n",
    "        \n",
    "        # find the term with the best mmr score\n",
    "        selected_index, score = max(mmr_scores, key=lambda x: x[1])\n",
    "\n",
    "        # update the selected indices and the remaining indices appropriately\n",
    "        selected_indices.append(selected_index)\n",
    "        remaining_indices.remove(selected_index)\n",
    "    \n",
    "    # get the terms that correspond to the selected indices and return\n",
    "    chosen_terms = []\n",
    "    for index in selected_indices:\n",
    "        chosen_terms.append(possible_terms[index])\n",
    "    return chosen_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess query, enhance, expand, and filter the expanded terms to make sure only relevant expansions are added\n",
    "def preprocess_query_diversity(query):\n",
    "    # do the context aware preprocessing\n",
    "    tokens, context_sensitive_terms = preprocess_query_context_aware(query)\n",
    "\n",
    "    # find the embedding of the query\n",
    "    query_embedding = calculate_term_embedding(query)\n",
    "\n",
    "    # find the diverse terms\n",
    "    diverse_terms = apply_maximal_marginal_relevance(query_embedding, context_sensitive_terms)\n",
    "\n",
    "    print(diverse_terms)\n",
    "\n",
    "    # return the expanded query\n",
    "    return ' '.join(tokens) + \" \" + ' '.join(diverse_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Metal', 'wanna', 'rockers', 'prefer', 'metals', 'folksters']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'want rock metal Metal wanna rockers prefer metals folksters'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_query_diversity('i want rock not metal')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
